import math as Math
import requests
from bs4 import BeautifulSoup
import threadpool
import time
import os
from threading import Lock, RLock
import traceback
import re

ids = []
File_txt = open("./Newspaper_DateLocationTitle.txt")
Lines = File_txt.read().split('\n')
for line in Lines:
    #print(line)
    #print(line.split('.')[0])
    ids.append(line.split('.')[0])
print(ids)



lock = RLock()


def get_one_page(url):  # 获取需要爬取页面的json信息
    time.sleep(2)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:  # 保证请求得到了正常响应
            return response.json()  # 返回json格式数据，含有需要抓取的信息
        return None
    except Exception as e:
        print(e)
        print(traceback.format_exc())
        return None


def mains(i):
    # 限制一定的抓取速度
    url2 = ''
    try:
        text = get_one_page(url2)  # 获取json格式的数据
    except Exception as e:  # 如果请求出现异常，就放弃这次循环并且打印错误信息，开始下一个循环
        print(e)
        print(traceback.format_exc())
        return
    print(text)
    for result in text["records"]:
        try:
            #   a += 1 #编码从1开始
            print(result)
            result = result["records"][0]
            date = result["date"]
            pubLocation = result['pubLocation']
            pubTitle = result['pubTitle']
            id = result['id']['id']  # 获取图片连接拼接中的id信息
            lock.acquire()
            if str(id) not in ids:  # 判断当前要下载的id是不是存在文件夹里面，如果不存在就进去下载
                bool = True
                ids.append(str(id))
            else:
                bool = False
            lock.release()
            if bool:
                # 下载完id的数据后，将id保存到ids列表里，防止如果出现不同请求但是数据一样时的重复下载
                # get_img(id, id)# 先不写入图片
                strs = "{}. date: {} pubLocation: {} pubTitle: {}".format(id, date, pubLocation, pubTitle)  # 拼接抓取后的文字信息
                print(strs)
                lock.acquire()
                with open('Newspaper_DateLocationTitle.txt', 'a+')as f:
                    f.write(str(strs) + '\n')  # 写入时间地点标题
                lock.release()
        except Exception as e:
            print(traceback.format_exc())
            print(e)
            return


if __name__ == '__main__':
    taskpool = threadpool.ThreadPool(30)
    request_list = []  # 存放任务列表
    name_list = [i for i in range(0, 2000000)]
    requestsz = threadpool.makeRequests(mains, name_list)
    for req in requestsz:
        taskpool.putRequest(req)

        # 等待所有任务执行完成
    taskpool.wait()
